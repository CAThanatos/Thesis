\chapter{Design}
\label{chapter:design}

\epigraph{I need to find a clever epigraph.}{--- \textup{Arthur Bernard}}

\minitoc[n] % minitoc without title

In this part of the manuscript, we aim at designing cooperative heterogeneous robots in a multirobots system by using evolutionary robotics. In particular, we are interested in the evolution of coordination behaviours and the influence of teams' genetic composition in the emergence of more efficient collective behaviours~\parencite{Waibel2009}. Namely, we want to study the impact of aclonal approaches~\parencite{Quinn2001} on (1) the capacity to evolve cooperation and (2) the efficiency of the cooperative solutions.

% as well as the evolution of specialization by way of genotypic polymorphism.

In the context of designing multirobots systems, our approach is to use evolutionary robotics. While using evolutionary processes for engineering is not new, this approach is still recent when it comes to designing robotic systems. As any given method, evolutionary robotics comes with assumptions and constraints. In this Chapter, we thus want to review the features (both negative and positive) of evolutionary robotics with regards to multirobots systems and motivate the choice of using this design method. First we give a quick overview of multirobots systems as well as their main advantages when compared to single robots. In particular, we emphasize on the design choices that come with creating those systems and what they imply. Additionally, we present a few applications of multirobots systems that are either seminal and/or noteworthy. Then we focus on the control of collective robotics systems. In particular, we briefly mention how the issue of control is addressed in single robots. We can thus compare with the way this matter is tackled in multiple robots. There has indeed been a strong interest in taking inspiration from single robots to apply the same solutions to multirobots. This is obviously not so simple. In this way, we also reveal the particular challanges brought up by the control of multiple robots. Next we talk about the use of machine learning, and in particular reinforcement learning, to the automatic design of multirobots systems. We briefly mention the main reinforcement learning techniques used in the context of single robots before discussing how these techniques have been transferred to multirobots engineering. Our goal is thus to shed some light on the advantages and limits of such approaches. We then move to evolutionary techniques and how they have been applied to the design of multirobots systems. We thus expose the main differences with traditionnl learning in this context and quickly review the main results obtained in the design of collective behaviours. Finally, we conclude by motivating the choice of studying evolutionary robotics for the design of cooperative robots.


\section{Multirobot Systems}

  
    % A RECASER ?

    % CEBOT and ACTRESS are often cited among the earliest successful MRS. CEBOT~\parencite{Fukuda1988} (for CELlular roBOTic system) is a decentralized architecture inspired by cellular organization. The organization of the system is dynamic and robots, which are coupled to one another, can reconfigure their structure under environmental changes. This system is based on a hierarchical organization where "master cells" (which are also robots) can communicate with other master cells and allocate subtasks to all of the agents in the system. In comparison, in ACTRESS~\parencite{Asama1989} (ACtor-based Robot and Equipments Synthetic System) the system is composed of three robots and three workstations. One of these workstations is operated by an human, another is used as image processing and the last one manages the environment. Given this heterogeneous group of six agents, the goal is for the robots to perform a purely collective task (i.e. that could not be achieved by a single robot) like pushing an object. This system raises the issue of achieving efficient communication between multiple levels of organization.

    % While it may seem counter-productive to develop and control several robots where a single robot could very well be sufficient, using a team of robots has several advantages among which:

    % MRS can be constitued of between two to a thousand of autonomous robots~\parencite{Rubenstein2014} depending on the task at hand.

    % However it is important to note that because applications vary greatly, MRS are very different in design.


  \subsection{General Properties} 

    Multirobots systems (MRS), or sometimes multi-agent robotics~\parencite{Dudek1996}, essentially gained fame during the 1980s. The main motivation was to use cooperation between autonomous robots in order to cope with tasks that a classical single robot may not achieve. Multiple advantages may be obtrained by using MRS~\parencite{Cao1997, Arkin1998} :

    \begin{itemize}
      \item{The parallel execution of multiple robots allows the task to be achieved faster.}
      \item{Using multiple robots can ensure robustness and reliability through \emph{redundancy}.}
      \item{It can be both cheaper and simpler to produce several simple robots compared to a single complex one (especially if the robot may suffer damages).}
      \item{It may be necessary to distribute several robots at the same time to complete the task, in which case a single robot would simply not be sufficient.}
    \end{itemize}

    This implies that there are several crucial properties that are expected of MRS~\parencite{Parker1994}. First, MRS are supposed to be \emph{adaptable}. This means that each robot is expected to react to environmental changes and, most importantly, to a change in others or induced by them. This also means that, in the lesser decentralized systems, the control system should adapt the global organization accordingly. Then, a MRS should be \emph{robust}. This implies that the system should not be too impacted by failure (and in particular individual failures). This is easier said than done but this is also one of the main advantages of such an approach. Because we rely on multiple agents, it is possible to design the system so that a fault on one or several robots does not critically impact the whole system. Finally, it is often expected of MRS to be fully \emph{autonomous}. This means that the system as well as all the agents that compose it should be able to act without human intervention. In particular, the system should be able to face the unexpected without human control for some time.

    From these properties, it stems that there is a range of tasks that are especially appropriate for MRS. While these tasks often relate to real-world applications, they mainly represent general domains in order to design proof-of-concepts on a particular aspect of a MRS~\parencite{Parker2000}. We can briefly draw a list of the main tasks on which MRS are studied~\parencite{Cao1997, Parker2000, Farinelli2004} :

    \begin{itemize}
      \item [Foraging] {In this sort of tasks, the goal is collect objects which are scattered in the environment. They then may or may not have to bring these back to a "home". Foraging refers to the real-world task of harvesting, toxic waste cleanup, search \& rescue. Most often, robots perform the task in a very independent manner, where the individual behaviour of a single entity does not really impact that of others. However, they may also rely on communication and \emph{stigmergy} in particular, i.e. indirect communication achieved by the previous modification of the local environment by an individual. Additionally, foraging tasks have also strong ties with a biological inspiration and the behaviour of eusocial insects in particular. A main challenge of foraging is for the robots to efficienctly explore the environment, i.e. without repeating each other actions.}

      \item [Box pushing] {The goal here is for several robots to collectively push an object. This object usually too big or too heavy for a single robot to move it alone and it thus requires the coordination of several individuals. A specific type of box pushing, cooperative manipulation, may require to robots to carry the objects to a destination (rather than pushing). Box pushing may not necessarily require that robots are aware of the others for the task to be achieved~\parencite{Sen1994}. }

      \item [Collective motion] {A popular task is to design robots that are able to move in a coordinated manner. This may imply that we simply desire robots to move together toward a path, as in the case of flocking, or that we want them to adopt a particular formation for the duration of the motion. This sort of task can often be accomplished by agents with minimal capabilities in terms of sensors, effectors and communication. One of the central issues studied in collective motion is the design of simple and local (i.e. individual-level) control rules that allow for the collective emergence of the desired behaviour.}

      \item [Traffic control] {Another common task is that of traffic control. This is a problem of multirobot path planning, where several individuals often their own personal goal. They must then coordinate with others in order to accomplish their goal without causing collisions or deadlocks. This is akin to a problem of resource conflict where robots have to share the environment with others.}

      \item [Monitoring] {Monitoring refers to the task of using multiple robots to observe and track a defined number of targets moving in the environment. Robots have thus to cooperate in order to ensure that all targets are monitored during the longest amount of time. In particular, their is a strong emphasis on coordination so that the agents can efficiently follow the targets and switch between them when necessary.}
    \end{itemize}

  \subsection{Architecture Choices}

    \begin{table*}[ht]
      \centerfloat
        \begin{tabular}{|l|c|c|c|c|c|c|}
          \hline
          \textbf{Control} & \multicolumn{3}{|c|}{Centralized} & \multicolumn{3}{|c|}{Centralized} \\
          \hline
          \textbf{Team composition} & \multicolumn{3}{|c|}{Homogeneous} & \multicolumn{3}{|c|}{Heterogeneous} \\
          \hline
          \textbf{Communication} & \multicolumn{2}{|c|}{Environmental} & \multicolumn{2}{|c|}{Passive} & \multicolumn{2}{|c|}{Intentional} \\
          \hline
          \textbf{Group size} & \multicolumn{3}{|c|}{Small (\(< 10\))} & \multicolumn{3}{|c|}{Swarm} \\
          \hline
        \end{tabular}
        \caption{\textbf{Architecture choices in multirobot systems.}}
      \label{table:architectureChoices}
    \end{table*}

    Because applications vary greatly, there is no canonical architecture for MRS~\parencite{Cao1997, Parker2008}. There are some more popular choices that we will try to highlight here but mainly one can use what best fits his/her needs. Those main design choices w.r.t. architecture are the following (see also Table~\ref{tab:architectureChoices} for a quick summary) :

    \begin{itemize}
      \item [Centralization] {The control of a MRS can be \emph{centralized} or \emph{decentralized}. In a centralized architecture, a single agent is responsible for controlling the system. Thus while this agent has full knowledge other the whole system, it represents a critical point for failures. Therefore, this type of organization is rare in MRS and most use a decentralized approach~\parencite{Parker2008}. However, the work of D'Andrea on the development of the Kiva systems, where a large group of robots (hundreds) move in a wharehouse to bring the products to workers, is of note in this category. In particular, a central control was responsible for the coordination of all the robots. 

      In comparison, decentralized architectures can be of two types: \emph{hierarchical} or \emph{distributed}~\parencite{Cao1997}. In a hierarchical architecture, the system is locally centralized and some agents are in charge of a group of other agents to organize the task at hand. For instance, in one of the very first successful MRS, CEBOT~\parencite{Fukuda1988}, particular robots (called "master cells") can communicate with other master cells and allocate subtasts to all of the agents in the system. On the contrary, in a distributed system, all agents are equal w.r.t. control which, while robust, implies that it is harder to achieve coherence between every agents.}

      \item [Team composition] {It is possible to use \emph{homogeneous} or \emph{heterogeneous} groups of robots. In an homogeneous team, individuals are all identical in terms of both software (control) or hardware (morphology and sensors). In comparison, heterogeneous robots vary between one another on any or both of these aspects. In consequence, homogeneous teams are more resilient to failures as every agent has the same capabilities which decreases how much the system is impacted by the loss of one individual. However heterogeneous groups allow to benefit from differences between individuals to achieve more diverse behaviour, in particular when coordination is required.}

      \item [Communication] {We can mainly divide the type of communication implemented in MRS into three categories: \emph{environmental}, \emph{passive} and \emph{intentional}. Environmental communication refers to the indirect communication we briefly mentioned previously: stigmergy. This type of communication is thus limited by the capabilities of agents to perceive complex information from the environment. Passive communication is another type of indirect communication where the agents rely on their sensors to observe the actions of others in the group. Thanks to this sensory feedback, robots can interact with each other without needing direct communication. However, it shares the same limitations as with stigmergy. Finally, intentional communication refers to direct communication between the robots. This thus allows to exchange complete information between teammates.}}

      \item [Group size] {On this point, MRS are really diverse and the number of robots involved in a collective task can scale from two to a thousand~\parencite{Rubenstein2014}. A smaller group size usually means that it possible to design more morphologically complicated robots where each individual may have elaborate capabilities. On the contrary, when we are interested in bigger teams, individual capacities tend to decrease in favour of the collective complexity. In particular, large groups of robots are often referred to as \emph{swarms}~\parencite{Beni2005}. In this case, robots in a swarm often possess very basic sensory capabilities and may not achieve much on their own. The emphasis is put on the \emph{emergence} of collective functionnalities from individual interactions~\parencite{Kube1993, Parker2008}. This means that we expect to see the appearance of global collective complexity from the local interactions between agents of the swarm, something which is also known as \emph{self-organization}. More precisely, swarm robotics are based on the principle of superadditivity~\parencite{Parker2008}, where the whole result (collective behaviour) is better than a simple sum of all its parts (the agents' behaviours).}
    \end{itemize}

    In this manuscript we choose to study a particular instance of MRS. Namely, we focus on the design challenges of fully distributed MRS (i.e. decentralized) where a small group of robots are morphologically homogeneous. In this context, we are interested in the critical differences that come with using a team of homogeneous or heterogeneous robots w.r.t. control. Lastly, as communication is not the focus of our study, we do not explicitly implement the robots with particular communication capabilities. Therefore, if any communication takes place between individuals then it can be considered to be passive. The rest of this Chapter is to be understood in the context of these specific architecture choices.

  % \subsection{Open Challenges}

      % Je pourrai mettre un challenge qui est l'origine de la coopération et donc recaser ce que j'avais déjà écrit


  %   % One critical design choice we need to address given this thesis is team composition, which may be \emph{homogeneous} or \emph{heterogeneous}. In an homogeneous team, individuals are all identical in terms of both software (control) and hardware (morphology and sensors). In comparison, heterogeneous robots vary grealt between one another. Most works in MRS have often focused on homogeneous teams as it is more practical in terms of task allocation: because every agent is the same, they all can achieve the same tasks. In consequence it is also more resilient to failures. In comparison, it is more difficult for heterogeneous teams to achieve coordination~\parencite{Parker1994}. However, they can benefit from the differences between individuals to display more diverse coordination strategies. Designing heterogeneous systems gives rise to critical issues in the field of MRS~\parencite{Parker2008}:

  %   \begin{itemize}
  %     \item{How to achieve efficient communications between several different robots ?~\parencite{Jung2000}}
  %     \item{How to efficiently allocate tasks between agents with differing capabilities ?~\parencite{Parker2003}}
  %   \end{itemize}

  %   % À réfléchir: un bla bla sur les différentes façons de faire du task allocation et les différentes formes de communication ? Je pense pas que ça soit nécessaire mais si on a de la place... pourquoi pas ?

  % \subsection{The Origin of Cooperation} 

  %   One of the main problems at the heart of designing a MRS is: how to achieve cooperation ? In their popular (though now ancient) review of the field, Cao and colleagues~\parencite{Cao1997} deemed this as one of the prime research axis in MRS. McFarland~\parencite{McFarland1996} argued that the design of cooperative robots could fall into two broad categories that he considered to be the same for group behaviours in nature: \emph{cooperative behaviour} and \emph{eusocial behaviour}. While this classification is more than doubtful for natural cooperative behaviours, its biological validity is not of real importance here. The more important point here is that since nearly the beginning of MRS, there has been an interest in taking inspiration from natural social behaviours for achieving coopration. Although without direct biological analogy, Parker~\parencite{Parker2008} classified the design of multirobots cooperation in two similar categories: \emph{intentionally cooperative} systems and \emph{collective swarm} systems. This categorization entails different manners in which to ensure cooperation.

  %   % but [McFarland] was mainly basing his argument on the work of Tinbergen~\parencite{Tinbergen1953} -> Really ?

  %   The intentionally cooperative MRS mostly comprised systems where agents have high knowledge about the other individuals' presence. They are capable of acting in accordance to others' actions and capabilities and may use communication to coordinate. In these MRS that McFarland simply classified as "cooperative behaviours", he defined an agent to be selfish. This means that cooperation comes from the maximization of the agent own utility. This paradigm is often caracterized by a more direct approach to cooperation. In particular, there is careful design on the manner with which robots can coordinate. Moreover, this approach is often well suited for MRS dealing with groups of heterogeneous robots, where the origin of cooperation represents a challenge by itself. From this it stems that this type of MRS sometimes takes inspiration from \emph{distributed artificial intelligence} (DAI). This field is mainly concerned with the design of distributed systems of intelligent agents~\parencite{Cao1997, Panait2005} and is generally considered to be divided in two major areas of studies: distributed problem solving (DPS) and multiagent systems (MAS). To summarize quickly, DPS is mostly concerned with solving problems with several agents. As such, some of its problematics are common to MRS (e.g. task allocation). However, because in DPS agents are considered to always be cooperative and are usually disembodied, few works can really contribute to MRS. In comparison, in MAS agents are often rational and as such are not de facto cooperators. Thus there is a strong emphasis on the collective interactions between agents. This is a reason why there has been consistent interest in game-theoretic approaches with MAS~\parencite{Rosenschein1985}. Therefore, MAS carry some theoretical ground for achieving cooperation in MRS. However, some have argued that MAS are not rooted enough in the physical world for them to make a powerful contribution to MRS~\parencite{Cao1997, Farinelli2004}. In particular, perfect sensory information is often assumed in MAS which may hinder direct transfer to robotics. For this reason, research in DAI tend to consider the mechanisms of coordination behaviours as a black box~\parencite{Parker1994}.

  %   % We believe selfishness is not necessarily associated with intionnally cooperation ?


  %   On the other end of the spectrum lies collective swarm~\parencite{Beni2005}, also called collective robotics~\parencite{Kube1993, Parker2008}. Although it could be argued that any MRS is a particular instance of collective robotics, we will make here the distinction as to not generate confusion w.r.t. the litterature. In this approach, the MRS is often constituted of a high number of robots (at least several dozens) which are all homogeneous and distributed agents. Also called "reactive collective robotics", collective swarm takes inspiration from the behaviours and organization of eusocial insects~\parencite{Wilson1998, Werfel2014}~\footnote{A more thorough presentation of eusociality and the altruistic behaviours of eusocial animals is given in Chapter~\ref{chapter:model}. Consequently, there will not be an extensive discussion on this subject here.}. More precisely, the study of eusocial insects led to the creation of the field of Swarm Intelligence~\parencite{Bonabeau1999, Zoghby2013}. This field consists of heuristics designed to solve algorithmic problems by taking inspiration from the natural collective behaviours. Some of the most famous algorithms that stemed from Swarm Intelligence are ant colony optimization (ACO)~\parencite{Dorigo2004a}, for which the most classical problem is to find the best travel route in a traveling salesman problem, and particle swarm optimization (PSO)~\parencite{Kennedy1995}, where candidate solutions to an optimization problem are modeled as particles moving through the search space. The main goal of swarm robotics is to design a large colony of decentralized and self-organized robots capable of high flexibility and robustness.

  %   Agents in a swarm are as simple as possible and constituted of very basic sensory capabilities. In particular, direct communication between robots is often inexistent. Instead, they rely on \emph{stigmergy}, where indirect communication is achieved by looking at other individuals' modification of the environment (e.g. the use of pheromones by ants in the natural world). Additionally, robots in a swarm are often largely unaware of the actions and internal states of others, basing their knowledge on proximity information. This implies that robots are not capable of achieving much on their own. Their role is really to be a part of the collective. In particular, the main concept of swarm robotics is that of \emph{emergence}\footnote{It is interesting to note that swarm robotics and individual-based modeling, which we presented in Chapter~\ref{chapter:model}, share a lot of similar key concepts. In particular, the concept of emergent collective behaviours between agents is one that is central to IBM.}. This means that we expect to see the appearance of global collective complexity from the local interactions between agents of the swarm, something which is also known as \emph{self-organization}. More precisely, swarm robotics are based on the principle of superadditivity~\parencite{Parker2008}, where the whole result (collective behaviour) is better than a simple sum of all its parts (the agents' behaviours). In consequence, the design of a swarm is focused on creating simple local behavioural rules that should allow the whole system to act in a collective way. It is as if cooperation is a side effect resulting from individual behaviours. This is obviously complex to design and time must not be critical. However, it also means that agents are cheaper to produce, deploy and control. Also, thanks to self-organization, the tasks covered by swarm robotics often require little to no \emph{a priori} assumptions. At the inception of collective robotics, this conception was really different from the "classical" design paradigm in robotics and especially in AI which emphasized on high reasoning and higher-levels of cognition~\parencite{Bonabeau1999}. While examples of swarm robotics systems are numerous we can quickly name a couple. One of the first examples of successful swarm robotics on real robots was the \emph{Nerd Herd} by Matarić~\parencite{Mataric1995}. With a group of $20$ identical robots with very simple individual capabitilies (mainly detection of obstacles and other robots) and a set of pre-programmed behaviours, she created a system capable of collective behaviours of flocking, surrounding, herding and foraging. Another interesting example is that of Swarm-bot~\parencite{Mondada2004, Dorigo2004, Mondada2005}. The goal was to engineer a swarm of simple identical robots capable of using self-assembly to navigate accross rough terrain and achieve different collective tasks.

    % -> This is not always that simple. Genre en swarm on peut faire plein de trucs globalement et des fois ya du knowledge (je crois). Et le swarm peut être classé en MAS (voir Panait2005)
    % -> Globalement frontière blurry encore une fois. On va assez vite parler de swarms pour des trucs qui répondent pas aux principes de base des swarms


\section{Designing the Control of Collective Robots}

  We previously presented the readers with a general overview of the properties and choices that come with designing a MRS. Namely, we have discussed the \emph{what} and \emph{why} of multirobot systems. In this Section, we want to focus on the \emph{how}. More precisely, we are interested in the design techniques involved in creating a distributed multirobot system. Designing the control of a robot mainly depends on its situatedness, i.e. the complexity and uncertainty of the environment in which it operates~\parencite{Mataric2008}.

  Because robots are expected to cooperate and in some cases coordinate, there is a strong need for reactivity and adaptability in MRS~\parencite{Iocchi2001, Farinelli2004}. The fact that multiple individuals engage in a collective behaviour implies that the individuals act in a dynamic environment. In terms of single robot control, several main architectures have been studied.

  First, the \emph{deliberative approach}, or also referred to as the Sense-Model-Plan-Act architecture~\parencite{Albus1991, Iocchi2001, Mataric2008}. This has been the classical approach in robotics~\parencite{Nilsson1984} and AI for a long time because it is concerned with representing high reasoning capacities. The basic principle is that all sensory information is computed under the internal knowledge of the robot in order to plan and determine the next action. This means that these architectures are based on an internal represention of the world. However, planning is a classical problem in AI and is known to be time costly. Therefore, while this architecture would be the most efficient in a perfect world, the process of building a world representation and planning is time-consuming and lacks critical real-time reactivity. In particular, keeping an accurate internal representation up-to-date proves to be difficult in dynamic and noisy environment.

  % Citer des architectures délibératives classiques ? (genre STRIPS ou NOAH)

  In light of the complexity of the previous architecture was born the opposite stance: the \emph{reactive approach}~\parencite{Brooks1986}. In comparison to the deliberative approach, this architecture is not based on reasoning nor planning. Rather, there is a direct connection from sensors to effectors, inspired by the biological concept of stimulus-response. This architecture is usually constituted of a programmed set of rules which, given the sensory inputs, gives the desired output actions. This implies that reactive systems can achieve very fast computation and thus are convenient when quick reaction is necessary. However, as robots do not keep any representation of the world and most often do not store any information, they are basically myopic. This can be useful when \emph{a priori} knowledge of the environment is sufficient but does not fare well with uncertainty and novelty. In particular, it cannot improve on its capacities or learn from the world.

  Then, \emph{hybrid approaches} have been proposed, whose goal was to unite the best of both worlds. Namely, they attempt to combine the speed of response of reactive approaches and the optimal planning of delibarate approaches. Such architecture is basically constituted of three layers~\parencite{Mataric2008}. One layer is responsible for the reaction and execution of the robot, another layer is concerned with delibaration and planning and the third and final layer acts as an intermediate between the two others. In consequence, one of the main complexity in this approach is to design this last layer. It should indeed coordinate between the immediate needs dealt by the first layer and the more long-term decision of the second.

  Finally, the last architecture is the \emph{behaviour-based approach}~\parencite{Arkin1998}. This was mostly introduced and popularized by Rodney Brooks subsumption architecture~\parencite{Brooks1986}. In behaviour-based architectures, the robot control is constituted of several basic behaviours, which are organised in separate modules. In a similar way as a reactive approach, these behaviours are directly connected to the sensors and will activate according to a certain set of rules. However, in comparison to a purely reactive approach, these behavioural modules can keep a state as well as a representation of the world, allowing for higher reasoning and planning. Those modules are designed to interact with one another in order to collectively achieve the task at hand. Complexity is thus expected to emerge from the interaction of low-level behaviours. In consequence, behaviour-based architectures are efficient when the environment is dynamic and there needs to be adaptation from the robot but pure reactivity alone is not sufficient. These architectures are usually designed by a bottom-up approach, where behaviours are added incrementally as building blocks in an increasing complexity. 

  %One main challenge of this approach is to design the action selection, which is the process thanks to which the system will choose which behaviour choose from several~\parencite{Pirjanian2000}. Two popular ways to solve this problem are to either base the selection on a prefixed hierarchy between modules or to rely on a voting mechanism.

  % This approach shares similar features with the reactive approach.

  In the case of multirobots systems, there is an additional global level of control that needs to be taken into account. This global control is characterized as either \emph{reactive} or \emph{social deliberative}~\parencite{Iocchi2001, Carpin2001}. Those concepts are similar to those of single robot control. In a reactive MRS, each individual deals with environmental changes without the influence of a higher control. In consequence, there is no model of the environment in the system and each agent is expected to individually adapt. In comparison, in a social deliberative MRS, a global strategy will be planned so that the organization of the whole system (e.g. task allocation) can handle environmental changes. This type of MRS may have a global representation of the world shared between the agents but it is not necessary. It is important to note that the global architecture of the MRS may differ from that of the individuals (e.g. a social deliberative system may be composed of behaviour-based robots).

  Behaviour-based approaches are among the most used for robot control in MRS~\parencite{Arkin1998, Mataric2008, Parker2008}. The adaptability and simplicity of behaviour-based control is a critical advantage for creating cooperative tasks and achieving coordination~\parencite{Mataric1995, Iocchi2001}. For example, Parker proposed and developed the ALLIANCE architecture which she successfully implemented on real robots~\parencite{Parker1994}. The main constraint was to design a fault-resistant system of heterogeneous robots which could achieve coordination. This system is based on a subsumption architecture~\parencite{Brooks1986} with the addition of the concepts of behaviour sets and motivations. On the one hand, a behavior set is composed of low-level behaviours combined together to accomplish a particular task. On the other hand, the motivation of a set is used to know which behaviour set (i.e. action) to select based on environmental information. Then, Candea and colleagues developed the ART architecture~\parencite{Candea2001} in order to design a team of robots to participate in the RoboCup competition~\parencite{Kitano1997}. The RoboCup is an annual competition where teams of robots must compete in a soccer game, which thus presents several critical challenges for MRS (e.g. collaboration, robot control, reasoning) in an adversarial context. The ART architecture in particular was composed of a team of distributed heterogeneous robots which differed both in hardware and software. Robots could vote on team formation. They would then assign roles by communicating and evaluating the utility of a given role for a given robot. Finally more recently, in an article published in Science, Werfel et al. designed independent robots capable of building structures whose blueprint is given by a user~\parencite{Werfel2014}. They used a team of homogeneous robots, taking inspiration from the mount-building capabilities of termites. These robots could communicate through stigermy (i.e. indirect communication) and followed building rules automatically compiled by the system. The agents were fully reactive so that they could adapt to a change in the current structure (either from robot or human action). Given this last example, it is also interesting to briefly discuss where collective robotics (i.e. swarms) stand in terms of architecture. As previously said, the similarities between reactive and behaviour-based architectures with swarms are numerours. In particular, they both focus on low-level, local and individual rules so that a global collective can emerge. For these reasons, all collective robotics architecture, when they are not automatically designed (which we will talk about next), are behaviour-based~\parencite{Brambilla2012, Zoghby2013}.


\section{Learning to Cooperate}
\label{section:RL}

  In the previous Sections, we presented various approaches, both individual and global, that could be used to control a MRS. However, in the examples we presented, the systems were carefully designed with ingenious techniques so that they could act the way it was intended. For example, the work of Werfel and colleagues~\parencite{Werfel2014}, while definitely noteworthy and interesting for the scientific and engineering issues it deals with is mainly based on ingenious design rules. The system was designed so that robots could dynamically adapt to changes in the environment and still be able to perform the task at hand. However, this sort of robot design is not always easy. It is especially true in the case of MRS where a collective behaviour must emerge. Furthermore, this kind of systems may not fare well when facing uncertainty and varying environmental conditions. As robustness and adaptability are expected of a robot, there has been a considerable interest in developing learning methods for robotics. For example, in extension to the ALLIANCE architecture we talked about previously, Parker developed the L-ALLIANCE (for LEARNING-ALLIANCE) architecture~\parencite{Parker1994}. Robots could learn form past experiences and update their parameters (e.g. the propensity to choose a particular task) given their previous performances.

  Machine learning has always been a critical challenge in artificial intelligence. Thus it has naturally been applied to robotics~\parencite{Hertzberg2008}. Classical machine learning can be divided into three different categories: supervised, unsupervised and reward-based. While the goal in machine learning is generally to optimize performance (e.g. for classifiers), the emphasis in mobile robotics is that the robot may adapt quickly. As such, most of the litterature on learning in robotics has been focused on reward-based techniques~\parencite{Mataric2008}, most commonly referred to as \emph{reinforcement learning} (RL)~\parencite{Sutton1998}. RL rests upon the mathematical framework of markov decision processes (MDP)~\parencite{Bellman1957}. In RL a robot tries to learn an optimal policy (i.e. a sequence of actions depending on the states the robot is in) thanks to a value function. To put it more simply, learning is achieved through rewards and punishments attributed to the robot according to its actions. The general goal in RL is to estimate the value function. This value function corresponds to the expected value of a state given a certain policy.

  % In particular, RL is based on the model of markov decision processes (MDP)~\parencite{Bellman1957}. To summarize quickly, a MDP is constituted of a set of states $S$ as well as a set possible actions $A$ given each state. Additionally, a transition function $P_{a}(s,s')$ which, given a certain state $s$ and action $a$ at time $t$ represents the probability to be in state $s'$ at $t+1$. Finally, $R_{a}(s,s')$ represents the reward obtained by transitioning from $s$ to $s'$ thanks to action $a$. The goal of a MDP is to find the optimal policy $\pi$, where $\pi(s)$ indicates the action to choose in state $s$, which maximizes:

  % \[
  %   \sum_{t=0}^{\inf} \gamma^{t}R_{a_{t}}(s_t,s_{t+1})
  % \]

  % where $\gamma$ is a discount factor. 

  % In RL, the goal is generally to estimate the value function $V^{\pi}(s)$, which corresponds to the expected value of the state $s$ given that we follow the policy $\pi$ afterwards. This means that $V$ is obtained as follows:

  % \[
  %   V^{\pi}(s)=E_{\pi}\left\{\sum_{k=0}^{\inf} \gamma^k r_{t+k+1} | s_t = s \right\}
  % \]

  % where $E_{\pi}$ is the expected value if the agent follows the policy $\pi$. Alternatively, we also often define action-value function $Q^{\pi}(s,a)$ as the expected reward when starting from state $s$ and selecting the action $a$ and then following policy $\pi$:

  % \[
  %   Q^{\pi}(s,a)=E_{\pi}\left\{\sum_{k=0}^{\inf} \gamma^k r_{t+k+1} | s_t = s, a_t = a \right\}
  % \]

  The main RL method applied to robotics is temporal-difference (TD) learning~\parencite{Sutton1988, Bradtke1996}. Based on the principles of TD learning, two major algorithms have been developed: on-policy SARSA (State-Action-Reward-State-Action) and off-policy Q-learning~\parencite{Watkins1989}. Additionally, most RL techniques have theoretical proofs of convergence~\parencite{Panait2005}. As RL is not the subject of this thesis, we will not go more into technical details. What we have presented here is only a crude summary of RL in order to give sufficient context to the rest of our discussion. We point those interested by the subject to more exhaustive litterature~\parencite{Sutton1998, Deisenroth2011}.

  % Est-ce qu'il y a besoin d'un peu mieux expliquer où ça servirait à rien ?

  In the case of learning for multiple robots, learning is obviously a bit more tricky. In particular, other robots are often expected to be learning at the same time. At the very least, the learning process must take into account the presence of other dynamic agents. Yet the theoretical foundations behind MDPs rest upon the assumption that the environment is stationary (Markov's law)~\parencite{Littman1994, Parker2008}. Consequently, adapting RL methods to multiple robots is not trivial.

  However, there is an extensive litterature on learning in multiagent systems, of which some can be applied to MRS~\parencite{Stone2000, Yang2005, Panait2005}. In MAS, two sorts of learning can be used: \emph{team learning} or \emph{concurrent learning}~\parencite{Panait2005}. In the case of team learning, a single learner is used to learn the behaviours of the whole team. This does not mean that the team is necessarily homogeneous. In this case, as there is only a single learner, it is easier to transfer techniques and algorithms from classical RL. However, the fact that there are multiple individuals implies that the learning process faces a curse of dimensionality: the size of the states space increases with the number of agents. This also means that learning will be centralized which raises an issue in the case of fully distributed MRS. 

  In comparison, in concurrent learning every individual is an independent learner. As previously said, this means that because the environment is not stationary, Markov's law is violated. In consequence, new learning techniques must be designed for multiple agents. In particular, concurrent learning is faced with an additional complex challenge: \emph{credit assignment}, i.e. how to divide rewards. Because each individual learns independantly, how to efficiently distribute a reward which depends on the collective performance is not easy. There are mainly two different ways to address credit assignment. On the one hand, it is possible to simply divide equally the team reward between every individuals, which is known as global reward. This means that even if there are vast differences between individuals' performance, all will be equally rewarded. Consequently, this may slow down the learning process as agents may not have sufficiently individually tailored feedback to improve on their strategy~\parencite{Wolpert2001}. On the other hand, it is possible to adopt local rewards, where agents are rewarded based on their individual performance. However, this do not ensure that the individuals will cooperate and may lead to the emergence of selfish behaviours. Some argue that there may not exist a definite solution to assign credit~\parencite{Balch1999}. Concurrent learning also has to deal with the learning dynamics caused by multiple learners. In particular, other individuals are not merely dynamic objects but are also co-adaptating at the same time. This means that for a given learner, other individuals adapt to this learner. In return, the learner will adapt in reaction to the factthat others adapted too and so forth. This raises similar issues to what is studied in game theory and in evolutionay game theory in particular~\parencite{MaynardSmith1973, Fudenberg1998, Bloembergen2015}~\footnote{As a reminder, evolutionary game theory was previously described in Chapter~\ref{chapter:model}. In consequence, it will only be mentioned here.}. Consequently, some have been interested in the research of Nash equilibria in joint learning in MAS so as to find the best-response policies of all agents. In particular, there has been several studies on trying to apply Q-learning techniques in order to find optimal policies in stochastic games~\parencite{Littman1994, Claus1998, Bowling2003, Greenwald2005, Kapetanakis2005}.

  However, as big as the litterature on learning in MAS is, transferring reinforcement learning techniques from MAS to MRS still represents a challenge~\parencite{Yang2005}. In particular, while results in MAS offer interesting perspectives, MRS necessitate continuous actions and/or states spaces. This is something which is not that much studied in classical MAS. Additionally, robots must often act without complete knowledge about the environment or other individuals. Information is usually incomplete in MRS~\parencite{Yang2005, Fernandez2005}. To overcome these problems, and the issue of continuous spaces in particular, several different solutions have been proposed. For example Mataric proposed to extract the features from the learning space by reformulating states and actions into conditions and behaviours~\parencite{Mataric1997}. This way, the size of the spaces was greatly decreased. She also implemented shaping (i.e. decomposing a complex task into several simpler which are then learned in succession) in order to ease the learning process. In comparison, Fernandez and colleagues~\parencite{Fernandez2005} developed a learning MRS by discretizing the states space and then applying an algorithm to generalize from this discrete space. This particular algorithm, called ENNC-QL, is based on a supervised approximation of the value function. In the case of an adversarial MRS learning for soccer, Bowling \& Veloso~\parencite{Bowling2003} introduced GraWoLF (for Gradient-based WoLF). They used a policy gradient technique, which was proposed to overcome intractable and continuous states spaces~\parencite{Sutton2000} as well as WoLF (Win or Learn Fast), an algorithm to ensure convergence in the context of concurrent learning. Lastly, some have been interested in applying fuzzy logic (i.e. formal logic where truth values can take any real value between $0$ and $1$) to multirobots reinforcement learning. In particular, Gultekin \& Arslan~\parencite{Gultekin2002} proposed a modular-fuzzy algorithm with Q-learning where fuzzy sets were used to abstract the states and actions spaces. Furthermore, an internal model of the agents was built and used to estimate the action of each individual. 

  In conclusion, a copious amount of work has been dedicated to applying reinforcement learning techniques for MRS. However the complexity and dimension of MRS implies that RL needs to rely on approximations and make critical assumptions about the world in order to cope with these challenges~\parencite{Yang2005, Parker2008}. This means that, while RL may be appropriate for single robot learning, MRS could benefit from a different framework to achieve automatic design. This is for these reasons that what we are interested in here is evolving robot control.

  %  by converting intermittent feedback into a continuous signal (à propos Mataric and shaping) -> wat

\section{Evolving Cooperative Robots}

  A more recent technique for the automatic design of robots is evolutionary robotics (ER)~\parencite{Nolfi2000, Doncieux2015a}. Again we will not go here into the details of how the framework functions as it was already covered in the Introduction. Rather, we are interested in quickly reviewing the use of ER as a design method for engineering robots and MRS in particular. The key idea behind ER is to apply concepts of evolutionary computation to the design of robots. This means implementing concepts of selection and variation to create robust and adaptable robots. After the previous Sections, it should be clear that numerous design techniques in robotics have been in part inspired by biology. For example the reactive controllers are inspired by the concept stimulus-response~\parencite{Brooks1986}, swarm robotics took inspiration from the collective behaviours of eusocial insects~\parencite{Bonabeau1999} and some major advancements in reinforcement learning mimic natural cognitive processes~\parencite{Montague1996}. Thus it should not come as a surprise that some would take inspiration from evolution for the design of complex machines. In particular, ER uses evolution to approach robotic design in an holistic manner. The robot is considered as a whole and the evolution of its behaviour results from the interaction with the environment (and the other individuals in the case of MRS): ER works on embodied agents. In particular, a minimum of assumptions have to been made when using ER to design a robot~\parencite{Bongard2013a}.

  Before we proceed further, we must clarify an important point of terminology. At its core, ER can be considered as a learning technique. However, it may be troublesome to classify ER among learning algorithms. Indeed, while ER is a learning process in the machine learning sense of the word (i.e. a process which improves and optimizes candidate solutions according to a certain goal), it is not the case in a more biological sense. In particular, there is a major difference in terms of time scales between learning and evolution: evolution is a phylogenetic adaptation while learning is an ontogenetic adaptation. Thus there sometimes is a confusion between the machine learning conception of learning and that of biology. This difference is even more critical now that combining evolution and learning represents an open issue in the field~\parencite{Urzelai2001, Mouret2014, Doncieux2015a}. Here we thus are careful to use the latter (i.e. biological) definition of learning and refer more precisely to reinforcement learning in this case.

  While on the subject of learning, it is important to note that ER and RL share several similarities~\parencite{Whiteson2012, Doncieux2015a}. In both frameworks, the goal is for a robot to evolve (or learn) a behaviour (which is akin to a policy in RL) which maximizes a particular value: rewards in RL or fitness in ER. In particular, we can compare ER to a direct policy search in RL~\parencite{Kober2013} because it does not focus on finding an estimation of the value function of the states and actions. In comparison, ER only exploits the global value (i.e. fitness) of a policy. Yet, when compared to RL, evolutionary robotics often necessitates higher computational time in order to find a good solution. In particular, the convergence of RL is proven and dynamic programming is guaranteed to find an optimal policy in polynomial time~\parencite{Littman1994, Whiteson2012}. In comparison, ER may have to evaluate an exponential number of candidates. However, ER has also several advantages over reinforcement learning. First, ER methods work very well under partial observability. In particular, they are not constrained by Markov's law (see previous Section) in finding a good solution. While the field of RL is also concerned with partially observable markov decision processes~\parencite{Jaakkola1994}, this implies that ER may be more suitable for the design of robot control in the face of uncertain environments. ER is also more appropriate in dealing with problems that would require continuous or a large number of states in RL (as it is the case for MRS). Again, as previously discussed, RL may be able to deal with this kind of problems. However, it implies that the problem's definition must be approximated before more classical RL algorithms can be applied. In comparison, ER explores the space of behaviours rather than that of states, which makes it a better solution when there is an explosion of the size of the states spaces~\parencite{Panait2005}. Finally, evolutionary robotics does not require to build a complex representation or a state-action space because it evolves its own representation. It is interesting to note that, because of the advantages of ER when compared to RL, there is also a copious litterature (which we only mention here) on trying to include certain concepts of evolutionary computation to reinforcement learning~\parencite{Whiteson2012}.

  While ER has been mainly focused on the design of single robots~\parencite{Doncieux2015a}~\footnote{As the focus of this manuscript is on multirobots systems, we will not discuss the litterature on the subject of ER for single robots. Interested readers should direct their attention towards more extensive reviews of the field~\parencite{Floreano2008, Bongard2013a, Trianni2014, Doncieux2015a}.}, its potential for the engineering of complex collective systems is well known~\parencite{Baldassarre2003}. In particular, as we previosly explained, ER is more easily scalable than classical RL techniques. It also has the advantage of allowing to design automatically complex social behaviours, something that would not be easily doable by coding them directly~\parencite{Baldassarre2003}. Designing social behaviour in ER is still new but multiple noteworthy research have been conducted in the past decades. Among the more ancient examples, we can refer to the famous work of Reynolds~\parencite{Reynolds1992} and its "boids". He developed a simulation of artificial creatures who evolved coordinated group motion as well as obstacle and predator avoidance. This work had a strong biological inspiration from the collective behaviours of schools of fishes. Then, the deep roots of ER in biology has naturally led to applying evolution to the design of swarms~\parencite{Brambilla2012, Francesca2016}. Designing a swarm would oftentimes require a tedious back and forth between the handcrafted individual behaviours and the observed collective behaviour. In comparison, evolving these individuals behaviours according to the fitness of the swarm performance is easier. In particular, the evolution of swarms has been investigated in the Swarm-bot project~\parencite{Mondada2005} from which we previously talked about. In particular, Baldassarre and colleagues~\parencite{Baldassarre2007} developed an artificial algorithm which evolved coordination behaviours between up to $36$ connected robots. These robots negociate the common direction of motion and then get together to the desired goal even under uncertain environmental factors. They showed that evolved robots were capable of high adaptability and generalization under various environmental conditions: number of robots, shape of the swarm, variation in the rigidity of the robots' connections, rough terrain and robots connected through a passive object. The collective behaviours resulted from the evolution of individual behaviours, as per swarm robotics principles. On the subject of coordinated motion, Quinn et al.~\parencite{Quinn2003} were among the first to evolve controllers for a cooperative task in physical robots. While they used three robots and as such it may be far-stretched to consider this swarm behaviour, their work had the main features of a swarm: homogeneous, distributed and emergence of a collective behaviour. Their robots were capable to perform formation-movement as well as adopting distinct roles between each others. Furthermore, the robotic agents were equipped with very limited sensory capabilities. More recently, Hauert and colleagues~\parencite{Hauert2014} evolved a group of twenty simulated flying robots with the task of establishing a communication network. More precisely, they designed a task where a rescuer launches twenty autonomous robots from his position. The robots have then to coordinate to find the other rescuer and set a communication link between the two rescuers that has to be maintained for up to thirty minutes. As the communication range of the robots is way shorter than the distance between the two rescuers, they have to cooperate to create a communication link. 

  %In particular, there has been different works both on the competitive and cooperative side of social behaviours. On the competitive side, most of the work is focused on competitive co-evolution~\parencite{Floreano1998, Floreano2008}. The biological inspiration behind competitive co-evolution is that several species (e.g. two in the simplest models) will be in competition for survival. This means that changes in one particular species may lead to adaptative change in the other species which again may lead the former species to adapt to these changes. In the context of evolutionary robotics, competitive co-evolution can lead to competitive improvements where both "species" are incrementally improving their behaviour in response to the improvements of that of the other species. This incremental process can give the possiblity to shape the adaptation of the robots towards more and more complexity without having to specifically design the fitness or the selection process to that end. The classical example of competitive co-evolution is that of the predator-prey model~\parencite{Floreano1997}. In this model, inspired by the dynamics of the Lotka-Volterra predator-prey equations~\parencite{Yorke1973}, a species of predators and a species of prey are co-evolved in the same environment. In consequence, the prey as to adapt to the predator's strategy in order to escape it and the predator has to adapt to that of the prey to be able to go on catching prey. Nolfi \& Floreano studied this co-evolution problem in ER and showed that the best evolved performance for a predator strategies was the one obtained when co-evolving both the prey and the predators~\parencite{Nolfi1998}. However, while these give really interisting pratical and theorical insights on the evolution of complex behaviours, work on competitive co-evolution are still few. 

  In a different manner, some have been interested in the evolution of swarms with online methods. In the "classical" framework of evolutionary robotics, the evolutionary algorithm is called offline. This implies that there are two distinct phases in the development of a robot: the design phase (i.e. evolution of controllers) and the operational phase (i.e. deployment of robots)~\parencite{Doncieux2015a, Francesca2016}. This makes the assumption that the environment where the robots are deployed is the same that the one where they were evoled. Or at least it considers that the evolved controller will be capable of adapting to the new environment conditions. In comparison, in online ER the design process is done directly in the operation environment. For multiple robots, this gave rise to distributed online evolutionary robotics, often called \emph{embodied evolution}~\parencite{Ficici1999, Watson2002}. We already mentioned the work of Montanier \& Bredeche~\parencite{Montanier2011, Montanier2013} in Chapter~\ref{chapter:model} who, based on this framework, studied the evolution of altruistic cooperation. Most notably, they observed the evolution of altruism in a of tragedy of commons situations~\parencite{Hardin1968} and its relation with genetic relatedness and dispersion. While this work is at the frontier between model and design, it can clearly give insight into the development of cooperative robots in embodied evolution.

  Some have also been interested in the evolution of specialization (or division of labour). Specialization means that the individuals divide between several roles, either to achieve a task more efficiently or to achieve multiple tasks at the same time. For example, Ferrante and colleagues~\parencite{Ferrante2015} have investigated the behaviours of leafcutter ants in a task were robots had to bring leaves into the nest. More precisely, robots could either adopt a specialist or generalist behaviour. When specialized, some robots cut the leaves and put them into a temporary storage area and others take the leaves from the storage area to get them to the nest. Generalists would do both. They showed that they could evolve division of labour when environmental conditions (the presence of a slope) made the presence of specialists more interesting.

  % Ca fait peu quand même niveau gens qui s'intéressent à division of labour. Floreano met le truc de Danesh avec les fourmis en ER. A investiguer.

\section{Conclusions}

  Our goal in this Chapter was to give a general presentation of multirobots systems and to discuss the different methods with which they could be designed. As such, we wanted both to introduce the problem addressed in the next Chapters and motivate our choice with regards to using evolutionary robotics. We showed that the design of MRS, as for robotics in general, is complex and may be facilitated by resorting to automatic design. However, this is not an easy task. In particular, the learning methods (i.e. RL) which may work for single robots do not cope well with the complexity of MRS. In consequence, evolutionary robotics appears as an interesting choice in this context.

  But several critical design decisions impact the efficiency of ER when applied to robotics. In particular, the selection of individuals can act at the level of the \emph{group} or that of the \emph{individual}. Additionally, whatever the level of selection groups of robots can be \emph{homogeneous} or \emph{heterogeneous}, something we already mentioned in previous Sections. Both these decisions entail critical consequences on the evolution of cooperative actions~\parencite{Waibel2009}. First, the level of selection caracterizes how fitness is distributed to each individual. This is similar to the issue of credit assignement we briefly mentioned in Section~\ref{section:RL}. In particular, depending on the level of selection, the fitness value is given to the whole group of individuals or to each individual separately. In comparison, the distinction between homogeneity and heterogeneity refers to the composition of the group. As we previously said, the decision on this matter is crucial to the design of any MRS~\parencite{Quinn2003}. Homogeneity tends to facilite the evaluation of individuals as well as the emergence of cooperation: because fitness is attributed to the group, they all benefit from the actions of others. In particular, achieving coordination is thus easier. In comparison, heterogeneous individuals may have conflicting interests and may act selfishly. But in return, it may by definition be easier to evolve heterogeneous behaviours. While there is an interest in ER for heterogeneous teams of individuals~\parencite{Lichocki2013}, most of the research concerned with evolving cooperation adopt an homogeneous approach (most often under group selection). In particular, a large part of the litterature is concerned with swarm robotics which, by definition, are constituted of homogeneous individuals.

  In the context of this thesis, we are interested in the evolution of cooperation among a group of heterogeneous robots. In particular, we want to focus on the nature of the coordination strategies evolved. Thus we consider teams of two genetically unrelated robots. While these two robots are morphologically identical, the heterogeneity in their control raises the issue of evolving cooperation when selfish behaviours can emerge. From this stems a tradeoff between using heterogeneity to evolve efficient coordination strategies and the difficulty of evolving cooperation. This issue is discussed in the first article. In a second article, we choose to focus solely on evolving a particular type of coordination: division of labour. Because we want this strategy to appear among heterogeneous individuals, we study the issue of achieving genotypic polymorphism. Namely, we want multiple different behaviours encoded by different genotypes to coexist at the same time in a single population.

  To conclude this introduction to this part of the manuscript, we want to clarify that, as in the first part of the thesis, we do not use real robots. We consider our contributions here to be mainly theoretical and act as general design concepts for others in the field. In particular, our robotic agents and settings are pretty simple so that we do not make much assumptions that would relate only to our study. Additionally, applying ER on real robots is still considered to be a challenge in the field~\parencite{Floreano2008, Doncieux2015a}. We believe that the recent developments aiming at either simplying experiments on physical robots or increasing transferability between simulation and reality to be beyond the scope of this here thesis. 

  % Faudra dire en quoi on est différent de Potter2001 sur le premier papier (c'est du compétitif, c'est la seule différence ?)

  % !!! Quinn a dit c'est peut-être bien l'aclonal !!!
