\section{Introduction}

  In this chapter, we aim at designing cooperative heterogeneous robots in a multi-robots systems by using evolutionary robotics. In particular, we are interested in the evolution of coordination behaviours and how the genetic composition of a team of robots is crucial in the emergence of more efficient collective behaviours. Namely, we want to study the influence of using aclonal approaches~\parencite{Quinn2001} in both the capacity to evolve cooperation and the efficiency of the cooperation solutions as well as the evolution of specialization by way of genotypic polymorphism.

  In the context of designing multi-robots systems, our approach is to use evolutionary robotics. While using evolutionary processes for engineering design is not new, this method is still pretty recent when it comes to designing robotic systems. Multiple benefits come from using evolutionary robotics but, as not method is perfect, this generates also several constraints and assumptions. We thus want to present the features (both negative and positive) of evolutionary robotics with regards to multi-robots systems (MRS) and motivate the choice of studying this particular design method. First we give a quick overview of multi-robots systems as well as their main advantages when compared to single robots. In particular, we emphasize on the design choices that come with creating MRS and their implications. We thus will be able to present a few applications of MRS that are either seminal or noteworthy (or both). Then we focus on the control of collective robotics systems. In particular, we mention how the issue of controlling a robotic system is resolved in the context of a single robot in order to compare with the control MRS. There has indeed often been a desire to take inspiration from what works in single robots to apply control multiple robots. This way, we can also reveal what are the new control challenges brought by MRS. Next we present the use of machine learning, and in particular reinforcement learning, to the automatic design of MRS. We quickly talk about the main techniques in reinforcement learning used for the design of single robots before discussing and how these techniques have been transfered for multi-robots engineering. We thus show the advatanges and limits of such approaches. We then present how evolutionary techniques have been applied to the design of MRS. We thus expose the main differences with traditionnal learning in this context and quickly review the main results obtained in the design of collective behaviours. Finally, we conclude by motivating the choice of studying evolutionary robotics for evolution of cooperative robots.


  \subsection{Cooperative Robots for Collective Tasks}

    \subsubsection{Multi-robots systems.} Multi-robots systems (MRS), or sometimes multi-agent robotics~\parencite{Dudek1996}, essentially gained fame during 1980s under the idea of having robots cooperation in order to cope with tasks that classical single robots sometimes could not achieve. CEBOT and ACTRESS are famously often cited among the earliest successful MRS. CEBOT~\parencite{Fukuda1988} (for CELlular roBOTic system) is a decentralized architecture inspired by cellular organization. The organization of the system is dynamic and robots, which are coupled to one another, can reconfigure their structure if the environment changes. This system is based on hierarchical organization where "master cells" (which are also robots) can communicate with other master cells and allocate subtasks to all the agent in the system. In comparison, in ACTRESS~\parencite{Asama1989} (ACtor-based Robot and Equipments Synthetic System) the system is composed of three robots and three workstations. One of these workstations is operated by an human, one is used as image processing and the last one manages the environment. Given this heterogeneous group of six agents, the goal is for the robots to perform a purely collective task (i.e. that could not be achieved by a single robot) like pushing an object. This is particularly interesting in designing efficient communication between any level of organization.

    MRS can basically be constitued of between two to a thousand of autonomous robots~\parencite{Rubenstein2014} depending on the task at hand. While it may seem counter-productive to develop and control several robots where a single robot could very well achieve the task, using a team of robots has several advantages among which~\parencite{Cao1997, Arkin1998}:

    \begin{itemize}
      \item{The parallel execution of multiple robots allow the task to be achieved faster.}
      \item{Several robots can ensure robustness and reliability through \emph{redundancy}.}
      \item{It can be both cheaper and simpler to produce several simple robots that a single complex one (especially if the robot is expected to suffer destruction).}
      \item{It may be necessary to distribute several robots at the same time to achieve the task, in which case a single robot could obviously not.}
    \end{itemize}

    This implies that there are several crucial features that are often expected of MRS~\parencite{Parker1994}. But it is important to note that because applications vary greatly, MRS are really different in design. This means that some of these features may not be present depending on the task at hand or the architectures adopted. First, MRS are supposed to be \emph{adaptable} and \emph{flexible}. Most often, this adaptability is found at the level of the agent, where we expect robots to react to a change in the environment and, most importantly, to a change in or induced by the other robots. This adaptability also means that, in the less decentralized systems, the control system should adapt the global organization accordingly. Then, a MRS should be \emph{robust}. This basically means that it should that the system should not be too much impacted by any failure. This is easier said than done but, as we previously said, this is also one of the main advantages of such an approach. Because robots are expected to be autonomous, it is possible to design the system so that a fault on one or several robots does not critically impact the whole system. This also means that in system such as CEBOT where some robots (in this case the "master cells") have a particular as organizers, achieving full robustness is harder and necessitates that such happening is anticipated during the design. And because MRS can be used to conduct several tasks as the same time, robots must also be able to allocate the tasks according to the actual functionning robots. Finally, it is often expected of MRS to be fully \emph{autonomous}. This means that the system as well as all the agent from which it is comprised should be able to act without human intervention. In particular, the system should be able to face the unexpected without having to wait for human control for a long time. Those features mean that MRS are most adequate in several classical robotics tasks. The most famous tasks include: foraging, object transportation, localization and mapping or path planning~\parencite{Farinelli2004}.

    As we previously said, applications vary greatly. Thus there is no canonical architecture for a MRS but rather different paradigms to choose from~\parencite{Cao1997, Parker2008}. First, the control of a MRS can be \emph{centralized} or \emph{decentralized}. In centralized architecture, a single agent will be responsible for controlling the system. This means that, while this agent has full knowledge other the whole system, which should theoretically allow for efficient control, this agent represents a critical point for failures. Therefore, the MRS which follow this sort of organization are rare~\parencite{Parker2008} and most MRS use a decentralized approach. Decentralized architectures can be of two types: \emph{hierarchical} or \emph{distributed}~\parencite{Cao1997}. In a hierarchical architecture, the system is locally centralized some agents will be in charge of a group of other agents to organize the task at hand. In comparison, in a distributed system, all agents are equal in control which, while robust, implies that it is harder to achieve coherence between every agents with relation to the task to accomplish.

    Another important design choice in term of architecture (and which is greatly relevant with relation to our problem) is on the subject of team composition: \emph{homogeneous} or \emph{heterogeneous}. In an homogeneous team, individuals are all identical in terms of both control (software) and morphological and sensory capabilities (hardware). In comparison, heterogeneous robots vary in one or the other. Most works in MRS have often been focused on homogeneous teams (TD: peut-être vérifier si récemment c'est toujours le cas) as it more practical in term of task allocation: because every agent is the same, they can all achieve the same tasks. Thus it is also more resilient to failures. In comparison, heterogeneous teams are obviously more complex in term of achieving coordination~\parencite{Parker1994}. However, they can benefit from the differences between the individual to display more diverse coordinative strategies. Designing heterogenous systems rise problems which are critical in the fields of MRS~\parencite{Parker2008}:

    \begin{itemize}
      \item{How to achieve efficient communications between several different robots ?~\parencite{Jung2000}}
      \item{How to efficient allocate tasks between agents with differing capabilities ?~\parencite{Parker2003}}
    \end{itemize}

    % À réfléchir: un bla bla sur les différentes façons de faire du task allocation et les différentes formes de communication ? Je pense pas que ça soit nécessaire mais si on a de la place... pourquoi pas ?

    \subsubsection{The origin of cooperation.} Obviously, one of the main problematics in MRS is: how to achieve cooperation ? In their popular (though now ancient) review of the field, Cao and colleagues~\parencite{Cao1997} correctly deemed this as one of the research axis in MRS (alongside group architecture and resource conflicts) under the name: the origin of cooperation. We can mainly decompose this problem in two very different approaches. McFarland~\parencite{McFarland1996} compared the design of collective actions in robots to a his analysis of group behaviours found in nature which he classifies in two categories (TD: vérifier que Cao ne le cite pas n'importe comment si possible): \emph{cooperative behaviour} and \emph{eusocial behaviour}. This crude classification is biologically doubtful but he was mainly basing his argument on the work of Tinbergen~\parencite{Tinbergen1953}. The validity of such classification is not real importance here and does not really impact our explanation. The really interesting point here for our argument is that since nearly the beginning of MRS, people have been interested in taking inspiration from natural social behaviours for achieving coopration. Although without direct biological analogy, Parker~\parencite{Parker2008} classified the design of multirobot cooperation in two similar categories: \emph{intentionally cooperative} systems and \emph{collective swarm} systems. This distinction entails different inspirations in how to ensure cooperation.

    The intentionally cooperative MRS are mostly composed of systems where agents have high knowledge of the other individuals' presence. They are capable of acting according to others' actions and capabilities and can sometimes coordinate thanks to diverse communication strategies. In this sort of MRS that McFarland said corresponds to cooperative behaviours, he deemed the agent to be selfish and to act in a cooperative fashion in order to maximize their utility. He was mostly refering to vertebrates. Under this "paradigm", there often is a very direct approach to cooperation, with a lot of emphasis and carefully designing the way robots coordinate. Moreover, this approach will often be well suited for MRS with groups of heterogeneous robots, where the origin of cooperation represents a real challenge in itself. From this it stems that this type of MRS somtimes take inspiration from distributed artificial intelligence (DAI). This field is mainly concerned with the design of distributed systems of intelligent agents~\parencite{Cao1997, Panait2005} and is generally considered to be divided in two major areas of studies: distributed problem solving (DPS) and multiagent systems (MAS). To summarize quickly, DPS is mainly concerned with solving problems with several agents. As such, some of its problematics are common to MRS. For example it shares the same concern for trying to efficiently achieve task allocation between multiple agents. However, because in DPS agents are considered to always be cooperative and are usually totally disembodied, few works could contribute to MRS. In comparison, in MAS agents are often rational and, as such, are not de facto cooperators. MAS are thus interested in the collective interactions between agents, sometimes with a strong emphasis on game-theoretic approaches~\parencite{Rosenschein1985}. Therefore, this has some theoretical interest for achieving cooperation in MRS. However, some have argued that MAS are not rooted enough in the physical world for them to make a powerful contribution to MRS~\parencite{Cao1997, Farinelli2004}. In particular, perfect sensory information is often assumed in MAS which may render difficult direct application in robotics. For this reason, research in DAI tend to consider the mechanisms of coordinative behaviours as a black box~\parencite{Parker1994}.

    % We believe selfishness is not necessarily associated with intionnally cooperation ?


    Nearly on the other end of the spectrum is collective swarm~\parencite{Beni2005}, often also called collective robotics~\parencite{Kube1993, Parker2008}. Although it could be argued that any MRS is a particular instance of collective robotics, we will stick here with this popular definition as to not generate confusion. In this approach, the MRS is often constituted a very high number of robots (at least several dozens) which are all simple homogeneous and distributed agents. Often also called "reactive collective robotics", agents are supposed to take inspiration from the behaviours and organization of eusocial insects and in particular from the order Hymenoptera (e.g. ants, wasps, bees) or Isoptera (e.g. termites)~\parencite{Wilson1998, Werfel2014}~\footnote{More precise description of the evolution of eusocial and the altruistic behaviours of eusocial animals has been discussed in Chapter~\ref{chapter:model}. Consequently, there will not be an extensive discussion on this subject here.}. More precisely, the study of eusocial insects led to the creation of the field of Swarm Intelligence~\parencite{Bonabeau1999, Zoghby2013}. This field consists of heuristics designed to solve algorithmics problems inspired by the natural behaviours found in eusocial insects. Some of the most famous algorithms that stemed from Swarm Intelligence are the ant colony optimization (ACO)~\parencite{Dorigo2004a}, for which the most classical problem is to find the best travel route in traveling salesman problem, and particle swarm optimization (PSO)~\parencite{Kennedy1995}, where candidate solutions to an optimization problem are thought as particles moving through the search space and interacting with other particles. The main goal of swarm robotics is to design a large colony of decentralized and self-organized robots capable of high flexibility and robustness.

    Agents in a swarm are as simple as possible and constituted of very simple sensory capabilities. In particular, direct communication between robots of a swarm is often inexistent. Instead, robots should rely on stigmergy, which means that indirect communication is achieved by observing the environment which may have been modified by another individual (e.g. the use of pheromones by ants in the natural world). Incidentally, in comparison to intentional cooperation, robots are often largely unaware of the actions and internal states of other individuals in a swarm, using rather mostly proximity information. This simplicity also means that robots are not capable of achieving much on their own and are expected to be useful only as part of the collective. In particular, the main concept of swarm robotics is that of \emph{emergence}\footnote{It is interesting to note that there are a lot of similarities in the philosophy between swarm robotics and individual-based modeling, which we touched upon in Chapter~\ref{chapter:model}. Mainly, the concept of emergence between behaviour-based agents is one that is central to IBM.}. This means that we expect to see the emergence of global collective complexity from the simple local interactions between agents of the swarm, something which is also known as \emph{self-organization}. More precisely, swarm robotics are based on the principle of superadditivity~\parencite{Parker2008}, where the whole result (collective behaviour) is better than a simple sum of all the parts (the agents' behaviour). Mainly, the design process of a swarm is focused on created simple local behavioural rules that should allow the whole system to act in the desired collective way. It is as if cooperation is a side effect of individual behaviours. This is obviously tricky and time must not be critical in that case but also implies that agents are in the cheaper to produce, deploy and control. Also, because of the self-organization that stems from emergence, the tasks covered by swarm robotics often require little to no a priori assumptions. At the birth of collective robotics, this was really different to the "classical" design paradigm in robotics and especially in AI which emphasized on high reasoning and higher-levels of cognition~\parencite{Bonabeau1999}. While examples of swarm robotics systems are numerous we can quickly name a couple. One of the first examples of successful swarm robotics on real robots was the \emph{Nerd Herd} by Mataric~\parencite{Mataric1995}(TD: special character ?). With a group of $20$ identical robots with very simple individual capabitilies (mainly detection of obstacles and other robots) and a set of pre-programmed behaviours, he was able to create a system capable of collective behaviours of flocking, surrounding, herding and foraging. Another interisting example is that of Swarm-bot~\parencite{Mondada2004, Dorigo2004, Mondada2005}. The goal was to engineer a swarm of simple identical robots capable of using self-assembly to navigate accross rough terrain and achieve different collective tasks.

    % -> This is not always that simple. Genre en swarm on peut faire plein de trucs globallement et des fois ya du knowledge (je crois). Et le swarm peut être classé en MAS (voir Panait2005)
    % -> Globalement frontière blurry encore une fois. On va assez vite parler de swarms pour des trucs qui répondent pas aux principes de base des swarms


  \subsection{The Control of Collective Robots}

    Because robots are expected to cooperate and in some case coordinate, there is a strong need for reactivity and adaptability in MRS~\parencite{Iocchi2001, Farinelli2004}. The fact that multiple individuals engage in a collective behaviour implies that the individuals act in a dynamic environment. To that end, the concept of \emph{situatedness} refers to the complexity and uncertainty that exists in the environment in which the embodied robot interacts~\parencite{Mataric2008}. The control of a robot mainly depends on its situatedness. In term of single robot control, several main architectures have been studied.

    First, the \emph{deliberative approach}, or sometimes also refered as the Sense-Model-Plan-Act architecture~\parencite{Albus1991, Iocchi2001, Mataric2008}. This has been the classical approach in robotics~\parencite{Nilsson1984} and AI for a long time because it is concerned with representing high reasoning capacities. The basic idea is that all sensory information is computed under the internal knowledge in order to plan and determine the next action. This means that these architectures are based on an internal represention of the world. This model of the world is usually constituted of a set of symboles and predicates on which logic can be applied to decide the next action. This implies that the first step of such architecture is to collect all data of the environment to build or update this model. Then, a planning phase is used to find the best possible plan given the task and finally act on this decision. Planning is a classical problem in AI and is known to be costly. Consequently, while we can understand how this architecture would be efficient in a perfect world, the process of building a world representation and planning is time consuming and lacks reactivity which is critical in real-world applications. In particular, keeping an accurate internal representation up-to-date proves to be difficult in dynamic and noisy environment.

    % Citer des architectures délibératives classiques ? (genre STRIPS ou NOAH)

    In light of the complexity of the previous architecture was born the opposite stance of \emph{reactive approach}~\parencite{Brooks1986}. In comparison to a deliberate approach, this architecture is based on no reasoning nor planning. Rather, its main principle is that there is tight connection between sensors and effectors, which is inspired by the biological concept of stimulus-response. This architecture is usually constituted of a programmed set of rules which, given the sensory inputs, gives the desired output actions. This implies that this type of architecture can achieve very fast computation and thus is very convenient in situations where quick reactivity is necessary. However, as robots do not keep any representation of the world or often do not store any information which means they are basically myopic. This can be useful when a priori knowledge of the environment is sufficient but cannot deal very well with uncertainty and novelty. In particular, it cannot improve on its capacities or learn from the world.

    Then, a \emph{hybrid approach} has been proposed, whose goal was to unite the best of both worlds, namely the speed of response of reactivite approach and the optimal planning of the delibarate approach. Such architecture is basically constituted of three layers~\parencite{Mataric2008}. One layer is responsible for the reaction and execution of the robot, another layer is concerned with delibaration and planning and the third and final layer which acts an intermediate between two other layers. In consequence, the main design complexity of this approach is on this last layer which is supposed to coordinate between the immediate needs that are dealed by the first layer and the more long-term decisions of the second layer.

    Finally, the last famous approach is the \emph{behaviour-based approach}~\parencite{Arkin1998}. This approach was mostly created and popularized by the subsumption architecture of Rodney Brooks~\parencite{Brooks1986}. This approach is sometimes closely tied with the reactive approach. In a behaviour-based architectures, the robot control in constituted of several basic behaviours, which are organised in separate modules. In a similar way as a reactive approach, these behaviours are directly connected to the sensors and will activate according to certain set of rules. However, in comparison with a purely reactive approach, these behavioural modules can keep a state or a representation of the world which allows for higher reasoning and planning. Those modules are all designed to interact with one another to collectively achieve the goals at hand. In that sense, there is a paradigm close to that of swarm behaviours. Namely, we expect global complexity to emerge from the interaction of local low-level behaviours. Behaviour-based architectures are thus efficient when the environment is dynamic and there needs to be adaptation from the robot but where pure reactivity alone is not sufficient. These architectures are usually designed by a bottom-up approach, where behaviours (which are the building blocks of the system) are added incrementally in an increasing complexity. Lastly, one principal challenge in this approach is to design the action selection, which is the process thanks to which the system will choose which behaviour choose from several~\parencite{Pirjanian2000}. Two popular ways to solve this problem is either to base the selection on a prefixed hierarchy between modules or to rely on a voting mechanism.

    In the case of multi-robots systems, they can be characterized by a higher level of architecture which refers to the global control of the system in response to the dynamics of the environment. Mainly, a MRS itself can be characterized as \emph{reactive} or \emph{social deliberative} in this respect~\parencite{Iocchi2001, Carpin2001}. Obviously, these concepts are very similar to the architecture of control in a single robot. In a reactive MRS, each individual deals with changes in the environment by itself without the influence of a higher control. Thus there is no particular model of the environment in the system and each agent is individually expected to cope with the changes and adapt. In comparison, in a social deliberative MRS, a global strategy will be planned so that the organization of the whole system (e.g. task allocation) can handle environmental changes. This implies that the system will come with a long-term plan to cope with these changes by using the resources available. This type of MRS may have a global representation of the world shared between the agents but it is not inevitable. It is important to note that the global architecture of the MRS may differ from that of the individuals (e.g. a social deliberative system may be composed of behaviour-based robots).

    Among all the architectures used to control each robot, the behaviour-based approach is the one used in most of the MRS~\parencite{Arkin1998, Mataric2008, Parker2008}. The adaptability and simplicity of behaviour-based approaches make them the architecture choice for creating cooperative tasks and achieving coordination~\parencite{Mataric1995, Iocchi2001}. For example, Parker proposed and developed the ALLIANCE architecture which she successfully implemented on real robots~\parencite{Parker1994}. The main principle was to be able to design a fault-resistant system of heterogeneous robots which could coordinate to accomplish a particular task. This architecture is based on a subsumption architecture~\parencite{Brooks1986} with the addition of the concepts of behaviour sets and motivations. A behavior set is composed of low-level behaviors put together to accomplish a particular task. The motivation of a set is used by the robot to know which behaviour set (i.e. action) select based for example on the fact that task is needed or on the fact that another robot is already doing the task. Then, Candea and colleagues developed the ART architecture~\parencite{Candea2001} in order to design a team of robots to participate in the RoboCup competition~\parencite{Kitano1997}. The RoboCup is an annual competition where teams of robots must compete in a soccer game, which requires to focus on several important challenges of MRS (e.g. collaboration, robot control, reasoning) in an adversarial context. The ART architecture in particular was composed of a team of distributed heterogeneous robots which differed both in hardware and software. These robots were able to vote on which team formation to use and then assign their role by communicating and evaluating the utility of a given role for a certain robot based on local information. Finally more recently, in an article published in Science, Werfel et al. designed independent robots capable of building a structure given by a user~\parencite{Werfel2014}. They used a team of homogeneous robots, taking insperation from the mount-building capabilities of termites. These robots could communicate through stigermy (i.e. indirect communication) and followed the building rules automatically compiled from the user structure. The agents were fully reactive so that they could adapt to a change in the current structure (either from robot or human action). Given this last example, it is also interesting to quickly tell where collective robotics (i.e. swarms) stand with relation to architecture. As previously said, the similarities between reactive and the behaviour-based architectures with the paradigm of swarm behaviours are numerours. In particular, they both focus on low-level, local and individual rules so that a global collective and complex behaviour can emerge, which ensures robustness and quick decision making. For these reasons, all collective robotics architecture, when they are not automatically designed (which we will touch upon next), are always behaviour-based~\parencite{Brambilla2012, Zoghby2013}.


  \subsection{Learning to Cooperate}
  \label{subsection:RL}

    In the previous subsections, we presented various architectures, both individual and global, that could be used to control a MRS. However, in the examples we presented, the systems were carefully designed with ingenious techniques so that the could perform the way it was intended. For example, the work of Werfel and colleagues~\parencite{Werfel2014}, while remarkable and interesting for the scientific and engineering questions it raises, rest upon clever design rules. The system was designed so that robots could dynamically adapt to changes in the environment and still be able to perform the task at hand. However, this sort of robot design is not always easy, especially in the case of MRS where we sometimes expect some collective behaviour to emerge from the cooperation of individuals. Furthermore, this sort of systems may not fare well when facing uncertainty and changing environmental conditions. As robustness and adaptability are expect of a robot, there has been a considerable interest in developing learning methods for robotics. For example, in extension to the ALLIANCE architecture we talked about previously, Parker developed the L-ALLIANCE (for LEARNING-ALLIANCE) architecture~\parencite{Parker1994}. Robots could learn form past experience and update their parameters (e.g. the propensity to choose to do a particular task) given their previous performances.

    Machine learning has always been one of the major subjects in artificial intelligence and has thus naturally been applied to the field of robotics~\parencite{Hertzberg2008}. Classical machine learning can be divided into three different machine learning categories: supervised, unsupervised and reward-based. While in machine learning, the goal is generally to optimize performance (e.g. of classifiers), in mobile robotics the emphasis is more to allow the robot to adapt quickly. As such, most of the litterature about learning in robotics has been focused on reward-based techniques~\parencite{Mataric2008}, most often under the term of \emph{reinforcement learning} (RL)~\parencite{Sutton1998}. The main principle of RL is that a robot will learn an optimal policy (i.e. a sequence of actions depending on the states the robot is in) thanks to a value function. To put it simply, the robot will learn thanks to rewards and punishments according to its actions. In particular, RL is based on the model of markov decision processes (MDP)~\parencite{Bellman1957}. To summarize quickly, a MDP is constituted of a set of states $S$ as well as a set of actions possible $A$ in each of the states. There is also a transition function $P_{a}(s,s')$ which, given a certain state $s$ and action $a$ at time $t$ represents the probability to be in state $s'$ at $t+1$. Finally, $R_{a}(s,s')$ represents the reward obtained by transitioning from $s$ to $s'$ thanks to action $a$. The goal of a MDP is to find the optimal policy $\pi$, where $\pi(s)$ indicates the action to choose in state $s$, which maximizes:

    \[
      \sum_{t=0}^{\inf} \gamma^{t}R_{a_{t}}(s_t,s_{t+1})
    \]

    where $\gamma$ is a discount factor. In RL, the goal is generally to estimate the value function $V^{\pi}(s)$, which corresponds to the expected value of the state $s$ given that we follow the policy $\pi$ afterwards. This means that $V$ is obtained as follows:

    \[
      V^{\pi}(s)=E_{\pi}\left\{\sum_{k=0}^{\inf} \gamma^k r_{t+k+1} | s_t = s \right\}
    \]

    where $E_{\pi}$ is the expected value if the agent follows the policy $\pi$. Alternatively, we also often define action-value function $Q^{\pi}(s,a)$ as the expected reward when starting from state $s$ and selecting the action $a$ and then following policy $\pi$:

    \[
      Q^{\pi}(s,a)=E_{\pi}\left\{\sum_{k=0}^{\inf} \gamma^k r_{t+k+1} | s_t = s, a_t = a \right\}
    \]

    As neither MDP nor RL are the subject of this manuscript, we will not go much more into the technical details of this technique. The last thing that is important to say here is that the main method developed in RL for robotics is that of temporal-difference (TD) learning~\parencite{Sutton1988, Bradtke1996} (TODO: quand même vérifier que je dis pas trop de conneries ici). Based on the principles of TD learning, two mains algorithms have been developed: on-policy SARSA (State-Action-Reward-State-Action) and off-policy Q-learning~\parencite{Watkins1989}. It is interesting to note that most RL techniques have theoretical proofs of convergence~\parencite{Panait2005}. What we have presented here is but a crude summary of RL to give sufficient context to the rest of our explanation. We point those interested by the subject to more exhaustive litterature~\parencite{Sutton1998, Deisenroth2011}.

    % Est-ce qu'il y a besoin d'un peu mieux expliquer où ça servirait à rien ?

    In the case of learning for multiple robots, this is obviously a bit more complicated. In particular, this means that other robots are learning at the same time or that at least the learning process must take into account the presence of other dynamic agents. Furthermore, this is against Markov's law which implies that in MDP the environment is stationary~\parencite{Littman1994, Parker2008}. Consequently, directly adapting RL methods to multiple robots is not that easy. However, there exists a large litterature about learning in multiagent systems, whose theoretical and practical findings can sometimes be applied to a robotic setting~\parencite{Stone2000, Yang2005, Panait2005}. 

    In MAS, two types of learning can be applied: \emph{team learning} or \emph{concurrent learning}~\parencite{Panait2005}. In the case of team learning, a single learner is used and is responsible for learning the behaviours of the whole team. This does mean the team is necessarily homogeneous. In this case, as there is only a single learning, it is easier to apply techniques and algorithms from classical machine learning. However, the fact that there are multiple individuals implies this time that the learning suffers from a curse of dimensionality: the size of the states space increases with the number of agents and is complicated to take care off. This also means that learning will be centralized which may pose a problem in the case of a fully distributed MRS. 

    In comparison, in concurrent learning, every individual is an independent learner. In this case, the system face the problem we talked about previously of violating Markov law on the fact that the environment must be stationary. This means that new learning techniques must be designed before they are applied to multiple agents. In particular, other individuals are not only dynamics in the environment but are also all co-adaptating at the same time. Which means that for a particular learner, other individuals will adapt with relation to it, and himself will adapt to the fact that the other individuals adapted and so forth. In comparison to team learning, concurrent learning is faced with a particular challenge which is called \emph{credit assignment}, namely how to divide rewards. Because each individual learns independantly, there is a question and how to efficiently distribute a reward attributed depending on the whole collective's performance. There are mainly two different ways to consider credit assignment. On the one hand, it is possible to simply equally divide the team reward between every individuals, something which is known as a global reward. This means that even if there is vast differences between individuals' behaviours, all will be equally rewarded. Consequently, this may slow down the learning process as agents may not have sufficient reward feedback to improve on their strategy~\parencite{Wolpert2001}. On the other hand, it is possible to adopt local rewards, where agents are rewarded based on their individual performance. However, this may lead to selfish behaviour because individuals have less incentive to help the collective. It has been shown that there may not exist a best way between global and local reward to assign credit~\parencite{Balch1999}. Concurrent learning also creates a particular dynamic between learners. Because, as we said, every learner is learning at the same time, it is not find an optimal behaviour. This poses a problem which is closed to what can be found in game theory and in evolutionay game theory in particular~\parencite{MaynardSmith1973, Fudenberg1998, Bloembergen2015}~\footnote{For an interested reader, evolutionary game theory has been described more complete in Chapter~\ref{chapter:model}.}. Consequently, some have been interested in the research of Nash equilibria in joint learning in MAS so as to find the best-response policies of all agents. In particular, there has been several studies on trying to apply Q-learning techniques to the realm of finding optimal policies in stochastic games~\parencite{Littman1994, Claus1998, Bowling2003, Greenwald2005, Kapetanakis2005}.

    However, as big as the litterature on applying learning in MAS is, the scaling of reinforcement learning techniques in MAS to MRS still represents challenge~\parencite{Yang2005}. In particular, while the results in MAS offer interesting perspectives, MRS suppose continuous actions and/or states spaces. This is something which is not that much studied in classical MAS. Also, there is the fact that robots must often act without complete knowledge of the environment or the other individuals. Information is generally incomplete in MRS~\parencite{Yang2005, Fernandez2005}. To overcome these problems, and most notably the problem of continuous spaces, several different techniques have been proposed. For example Mataric focused on extracting the features from the learning space by reformulating the states and actions into conditions and behaviours~\parencite{Mataric1997}. This way, the size of the spaces was greatly decreased. He also implemented shaping in order to ease the learning process by converting intermittent feedback into a continuous signal. In comparison, Fernandez and colleagues~\parencite{Fernandez2005} developed a learning MRS by discretizing the states space, based on the designer's decision, and then applying an algorithm to generalize from this discrete space. This particular algorithm, called ENNC-QL, is based on a supervised approximation of the value functions. In the case of an adversarial MRS learning in a soccer context, Bowling \& Veloso~\parencite{Bowling2003} introduced GraWoLF (for Gradient-based WoLF). They use the policy gradient technique, which was proposed to overcome intractable and continuous states spaces~\parencite{Sutton2000} as well as WoLF (Win or Learn Fast), an algorithm to converge when there is simultaneous learning. Lastly, some have been interested in applying fuzzy logic (i.e. formal logic where truth values can take any real value between $0$ and $1$) to multirobots reinforcement learning. In particular, Gultekin \& Arslan~\parencite{Gultekin2002} proposed a modular-fuzzy algorithm with Q-learning where fuzzy sets were used to abstract the states and actions spaces and overcome the challenge of managing the size of the spaces. Furthermore, an internal models of the agents is built and used to estimate the action of each individual. In conclusion, a copious amount of work exists in applying reinforcement learning techniques in MRS, however they need to rely on approximation and generalization algorithm in ordre to be able to cope with the size and complexity of MRS~\parencite{Yang2005, Parker2008}.

  \subsection{Evolving Cooperative Robots}

    Another more recent technique for the automatic design of robots is evolutionary robotics (ER)~\parencite{Nolfi2000, Doncieux2015a}. Again, as we said in Chapter~\ref{chapter:model}, we will not get here in the details of how a classical ER system functions as it was already covered in the Introduction. Rather, we want to quickly review the use of ER as a design method for engineering robots and MRS in particular. At its core, ER is the idea of applying concepts of evolutionary computation to the larger design of robots. This means using concepts of selection and variation for to create robust and adaptable robots. This should be clear after the previous Subsections that numerous techniques in robotics have been successfully inspired by biology. For example the reactive controllers are inspired from the notion stimulus-response~\parencite{Brooks1986}, swarm robotics is an artificial translation of the collective behaviours of eusocial insects~\parencite{Bonabeau1999} and some of the major advancements in reinforcement learning are deeply tied with the natural cognitive processes~\parencite{Montague1996}. So at this point of the manuscript it should come as no surprise take inspiration from evolution as it is actually still the best example in the design of complex machines. Evolutionary robotics has been based on these concepts in order to bring an holistic approach to robotic design. The robot is considered as a whole and the evolution of its behaviour results from the interaction with the environment (and the other individuals in the case of MRS): ER works on embodied agents. In particular, few assumptions have to been made when using ER to design a robot~\parencite{Bongard2013a}.

    At its core, ER can be considered as a learning technique. However, it may be risky to classify ER among learning algorithms. Indeed, while ER definitely corresponds to a learning process in the machine learning conception of the term (i.e. a process which will improve and optimize candidate solutions with relation to a goal), it is not the case in a more biological sense. In particular, there is a major difference between evolution which is a phylogenetic adaptation and learning as an ontogenetic adaptation as both work on very different time scales. Thus there sometimes seems to be a confusion between the machine learning conception of learning and that of biology. This difference becomes criticial with the recent challenges of trying to combine evolution and learning together~\parencite{Urzelai2001, Mouret2014, Doncieux2015a}. We will thus be careful here to use the latter (i.e. biological) definition of learning and will generally refer to reinforcement learning in particular in this case.

    On the subject of learning, ER and RL share several similarities~\parencite{Whiteson2012, Doncieux2015a}. In both techniques, the goal is for a robot to learn (or evolve) a behaviour (which is akin to a policy in RL) which maximizes a particular value: the reward in RL or the fitness in ER. ER can also be compared to the direct policy search algorithm in RL~\parencite{Kober2013} because it is not focused on trying to estimate the value function of the states and actions. In comparison, it is centered on using only the global value (i.e. fitness) of a policy. Yet, when compared to RL, evolutionary robotics often implies bigger computational time in order to find a good solution. In particular, while proofs of convergence exist for RL and dynamic programming is guarantee to find an optimal policy in polynomial time~\parencite{Littman1994, Whiteson2012}, ER may have to evaluate an exponential number of candidates. However, ER has also several advantages over reinforcement learning. First, ER methods can work very well under partial observability. In particular, they are not expected to follow Markov law (see previous subsection) to find a good solution. While there has been interest in studying reinforcement learning to partially observable markov decision processes~\parencite{Jaakkola1994}, this means that ER may be more fit for the design of robot control in the face of uncertain environment. Then, ER is also more suited for dealing with problmes that would require continuous or a large number of states in RL (as it is the case in MRS). Again, as we previously said in the previous examples, techniques in RL exist to deal with this kind of problems. However, they often imply complex techniques in order to approximate the problem's definition so that more classical RL algorithms can be applied. In comparison, ER is at its base capable of efficiently cope with this problem. More precisely, ER explores the space of behaviours rather than that of states, which makes it a better solution when there is an explosion of the size of the states spaces~\parencite{Panait2005}. Finally, evolutionary robotics does not require to define a complex representation and astate-action spaces because it is capable of directly evolving it own representation. It is also interesting to note that, because of these advantages, there is also a vast litterature (which we will not talk about here) on trying to combine tools from evolutionary computation to reinforcement learning~\parencite{Whiteson2012}.

    While ER has been mainly focused on the design of single robots~\parencite{Doncieux2015a}~\footnote{As the focus of this manuscript is on multirobots systems, we will not get into a detailed description of the litterature on the subject of ER for single robots. Interested readers should direct their interest towards more extensive reviews of the field~\parencite{Floreano2008, Bongard2013a, Trianni2014, Doncieux2015a}.}, its potential for the engineering of complex collective systems is well known~\parencite{Baldassarre2003}. As we previously said when comparing to RL, evolutionary robotics has indeed the advantage of being more easily scalables than what can be done in reinforcement learning. The design of social behaviours in ER is still new but several interesting research has been conducted in the past decades. In particular, there has been different works both on the competitive and cooperative side of social behaviours. On the competitive side, most of the work is focused on competitive co-evolution~\parencite{Floreano1998, Floreano2008}. The biological inspiration behind competitive co-evolution is that several species (e.g. two in the simplest models) will be in competition for survival. This means that changes in one particular species may lead to adaptative change in the other species which again may lead the former species to adapt to these changes. In the context of evolutionary robotics, competitive co-evolution can lead to competitive improvements where both "species" are incrementally improving their behaviour in response to the improvements of that of the other species. This incremental process can give the possiblity to shape the adaptation of the robots towards more and more complexity without having to specifically design the fitness or the selection process to that end. The classical example of competitive co-evolution is that of the predator-prey model~\parencite{Floreano1997}. In this model, inspired by the dynamics of the Lotka-Volterra predator-prey equations~\parencite{Yorke1973}, a species of predators and a species of prey are co-evolved in the same environment. In consequence, the prey as to adapt to the predator's strategy in order to escape it and the predator has to adapt to that of the prey to be able to go on catching prey. Nolfi \& Floreano studied this co-evolution problem in ER and showed that the best evolved performance for a predator strategies was the one obtained when co-evolving both the prey and the predators~\parencite{Nolfi1998}. However, while these give really interisting pratical and theorical insights on the evolution of complex behaviours, work on competitive co-evolution are still few.

    In comparison, much more emphasis has been put on evolving cooperation between multiple robots. As we previously showed in this Section, there are obvious advantages of designing cooperative robots in MRS (e.g. necessity of the task, more efficient, faster execution). ER has the advantage of allowing to design automatically complex social behaviours, something that would not be easily doable by coding them directly~\parencite{Baldassarre2003}. Among the more ancient examples, we can refer to the famous work of Reynolds~\parencite{Reynolds1992}, creator of the "boids". He developed a simulation of artificial creatures who evolved the capabilities of coordinated group motion as well as obstacle and predator avoidance. This work had a strong biological inspiration from the collective behaviours of school of fishes. The strong ties of evolutionary robotics with biology has naturally led to applying evolution to the design of swarms~\parencite{Brambilla2012, Francesca2016}. ER offers to possiblity to efficiently evolve the individual behaviours in a swarm without the tedious back and forth of between the designed individual behaviour and the desired collective behaviour. In particular, the evolution of swarms has been investigated in the Swarm-bot project~\parencite{Mondada2005} from which we talked previously about. In particular, Baldassarre and colleagues~\parencite{Baldassarre2007} developed an artificial algorithm capable of evolving coordination behaviours between up to $36$ connected robots. These robots negociate the common direction of motion and then get together to the desired goal even under the uncertainty of environment factors. Moreoever, they showed that the evolved robots were capable of great adaptability and generalization under various environmental conditions: number of robots, shape of the swarm, variation in the rigidity of the robots' connections, rough terrain and robots connected through a passive object. This collective behaviour only resulted from the evolution of individual behaviours, as per swarm robotics principles. On the subject of coordinated motion, Quinn et al.~\parencite{Quinn2003} were among the first to evolve controllers for a cooperative task in physical robots. While they used three and as such it may be far-stretched to consider this swarm behaviour, their work had the main features of a swarm: homogeneous, distributed and emergence of a collective behaviour. Their robots were capable to perform formation-movement as well as adopting distinct roles between each others. Furthermore, the robotic agents were equiped with very limited sensory capabilities. More recently, Hauert and colleagues~\parencite{Hauert2014} evolved a group of twenty simulated flying robots in the task of establishing a communication network. More precisely, they designed a task where a rescuer launch twenty autonomous robots from his position. The robots have then to coordinate to find the other rescuer and set a communication link between the two rescuers that has to been maintained up to thirty minutes. As the communication range for the rescuers and the robots is way shorter than the distance between the two rescuers, the robots have to cooperate to create the communication link. 

    In a different manner, some have been interested on the evolution of swarms with online methods. In the "classical" framework of evolutionary robotics, the evolutionary algorithm is an what we call an offline algorithm. This implies that there are two distinct phases in the development of a robot: the design phase (i.e. evolution of controllers) and the operational phase (i.e. deployment of robots)~\parencite{Doncieux2015a, Francesca2016}. This makes the assumption that the environment where the robots are deployed is the same that the one where they were evoled. Or at least it is considering that the evolved controller will be sufficiently adaptable to cope with a environment conditions. In comparison, in online ER the design process is done directly in the operation environment. For multiple robots, this gave rise to distributed online evolutionary robotics, often called \emph{embodied evolution}~\parencite{Ficici1999, Watson2002}. Based on this framework, Montanier \& Bredeche~\parencite{Montanier2011, Montanier2013} studied the evolution of altruistic cooperation. Most notably, they show the evolution of altruism in a situation of tragedy of commons~\parencite{Hardin1968} and its relation with genetic relatedness and dispersion. While this work is at the frontier between model and design, it can clearly give insight into the development of cooperative robots in embodied evolution.

    Some have also been interested in the evolution of specialization (or division of labour). Specialization means that individuals will divide between several roles, either to achieve a task more efficiently or to achieve several tasks at the same time. For example, Ferrante and colleagues~\parencite{Ferrante2015} have investigated the behaviours of leafcutter ants in a task were robots had to bring leaves into the nest. More precisely, robots could adopt either specialist behaviours, where some robots cut the leaves and put them into a temporary storage area and others took the leave from the storage area to get them to the nest, or a generalist behaviour (i.e. do both of the roles). They showed that they could evolve division of labour when environment conditions (the presence of a slope) made the presence of specialists more interesting.

    % Ca fait peu quand même niveau gens qui s'intéressent à division of labour. Floreano met le truc de Danesh avec les fourmis en ER. A investiguer.

  \subsection{Conclusions}

    % En fait je pense qu'il faut le mettre en conclusion après ça
    Finally, to conclude this Subsection on an important point on the design of cooperative robots in evolutionary robotics. In particular, several design choices have to be made on whether the selection happens at the level of \emph{group} or that of the \emph{individual}. Also, whatever the level of selection, the groups of robots can be \emph{homogeneous} or \emph{heterogeneous}, something we already mentionned in previous Subsections. In particular, there are different advantages with relation to the evolution of cooperation with each of these choices~\parencite{Waibel2009}. First, the level of selection will caracterize how the fitness will be distributed to each individuals. This is similar to the challenge of credit assignement (see Subsection~\ref{subsection:RL}). In particular, the level of selection defines if a single fitness value is given to the whole group of individuals or to each individual separately. In comparison, homogeneity and heterogeneity refers to the composition of the group. As we previously said, this choice of how to compose the group of individuals is crucial to the design of any MRS~\parencite{Quinn2003}. Homogeneity will tend to facilite evaluation of the individuals as well as the emergence of cooperation: as all the individuals are similar, they all have the same interest in cooperating. In comparison, heterogeneous individuals may have conflicting needs and may act selfishly. But in return, it will be easier to evolve different behaviours as would be desired if division of labour is needed for example. While there is an interest in ER in heterogeneous teams of individuals~\parencite{Lichocki2013}, in most of the research in the field, an homogeneous approach to the evolution of cooperation is taken (with generally group selection but it does not make much difference (TD: verifier ça dans le Waibel quand même)). In particular, as we saw, part of the litterature on cooperation in ER is concerned about swarm robotics which, by definition, are constituted of homogeneous individuals.


    -> Nous on fait pas du MAS: nos agents ne sont pas rationnels (Cao1997 (en gros))
          -> Mais on fait un peu du MAS quand même parce qu'on a un genre de perfect sensing
          -> Mais c'est evolutionary robotics justement pour s'écarter un peu des MAS
          -> Tout en restant en simulation. On peut avoir un argument du genre "meilleur des deux mondes". Surtout qu'il faudra avoir une discussion (rapide) sur simulation vs réel à un moment

    -> Bien justifier le fait qu'on fait du simulated robots (Nolfi en parle dans le handbook). Pourquoi on faire du vrai robot c'est chaud en ER ? Quels sont les problèmes pour passer de simul à vrais robots (reality gap) ? Et pourquoi dans notre cas on s'en fout un peu ? 
          -> Nous on s'en fout parce qu'on fait du théorique (en gros)

    Faudra dire en quoi on est différent de Potter2001 sur le premier papier (c'est du compétitif, c'est la seule différence ?)

    !!! Quinn a dit c'est peut-être bien l'aclonal !!!

    theoretical
